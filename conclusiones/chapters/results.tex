\section{Results}
\subsection{Overall accuracy by split and language}
Figure~\ref{fig:acc-eu} and Figure~\ref{fig:acc-ca} compare token accuracy of the reference \texttt{nltk} HMM and our custom HMM on train/dev/test for Basque and Catalan. This mirrors the generative view from class: both models estimate \(p(y)\) (tag transitions) and \(p(x\mid y)\) and decode with Viterbi. Basque shows a larger dev/test gap because its agglutinative morphology yields many inflected forms that become rare or unseen at training time; Catalan, being less morphologically complex, suffers less from emission sparsity.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{hmm_accuracy_eu.png}
  \caption{Basque: accuracy of NLTK HMM vs. custom HMM on train/dev/test.}
  \label{fig:acc-eu}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{hmm_accuracy_ca.png}
  \caption{Catalan: accuracy of NLTK HMM vs. custom HMM on train/dev/test.}
  \label{fig:acc-ca}
\end{figure}

\subsection{Comparison with n-gram baselines}
Backoff taggers encode bounded context (unigram/bigram/trigram), while HMMs model the joint \(p(x, y)\) combining transitions and emissions. Figure~\ref{fig:ngram} shows the gap between these families: the HMMs achieve higher accuracy because plausible tag transitions (e.g., ADJ\(\rightarrow\)NOUN) and word-tag likelihoods are scored together, aligning with the ``Colorless green ideas'' intuition discussed in class.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{ngram_vs_hmm.png}
  \caption{Token accuracy: default/unigram/bigram/trigram vs. HMMs (Basque split).}
  \label{fig:ngram}
\end{figure}

\subsection{Per-tag analysis}
Figures~\ref{fig:per-tag-eu} and \ref{fig:per-tag-ca} detail per-tag accuracies for our HMM. Closed classes (DET, ADP, AUX, PUNCT) remain robust because they are frequent and have stable transitions; open classes (NOUN, VERB, PROPN, ADV) are more error-prone, especially in Basque, where inflection explodes vocabulary size and sparsifies \(p(x\mid y)\). This matches the theoretical limitation of generative HMMs on unknown or rare forms.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{pos_accuracy_eu.png}
  \caption{Per-tag accuracy of our HMM on the Basque test set.}
  \label{fig:per-tag-eu}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{pos_accuracy_ca.png}
  \caption{Per-tag accuracy of our HMM on the Catalan test set.}
  \label{fig:per-tag-ca}
\end{figure}

\subsection{Qualitative checks}
The notebook includes Viterbi decoding examples and joint-probability computations. Well-formed sequences (e.g., ADJ\(\rightarrow\)NOUN\(\rightarrow\)VERB) obtain higher joint probability than anomalous orders, echoing the ``Colorless green ideas'' contrast from class. Random samples drawn from the HMM also show plausible tag alternations, illustrating that learned transitions encode morpho-syntactic structure rather than semantics.
