\section{Results}
\subsection{Overall accuracy by split and language}
Figure~\ref{fig:acc-eu} and Figure~\ref{fig:acc-ca} compare token accuracy of the reference \texttt{nltk} HMM and our custom HMM on train/dev/test for Basque and Catalan. Numerically, for Basque we obtain 0.9569/0.8258/0.8189 (train/dev/test) with NLTK and 0.9693/0.8622/0.8560 with our HMM; for Catalan 0.9703/0.9453/0.9430 versus 0.9761/0.9477/0.9445. The dev–test gap is larger in Basque (e.g., 0.8622\(\rightarrow\)0.8560) than in Catalan (0.9477\(\rightarrow\)0.9445), reflecting higher sparsity from agglutinative morphology.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{hmm_accuracy_eu.png}
  \caption{Basque: accuracy of NLTK HMM vs. custom HMM on train/dev/test.}
  \label{fig:acc-eu}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{hmm_accuracy_ca.png}
  \caption{Catalan: accuracy of NLTK HMM vs. custom HMM on train/dev/test.}
  \label{fig:acc-ca}
\end{figure}

\subsection{Comparison with n-gram baselines}
Backoff taggers encode bounded context (unigram/bigram/trigram), while HMMs model the joint \(p(x, y)\) combining transitions and emissions. On the Basque test set the n-gram taggers reach 0.859 (unigram), 0.866 (bigram) and 0.866 (trigram), slightly above our HMM (0.8560) but ahead of the NLTK HMM (0.8189). For Catalan, n-grams reach 0.921/0.935/0.935, clearly below the NLTK HMM (0.9430) and ours (0.9445). Figure~\ref{fig:ngram} visualizes these differences when combining plausible transitions (e.g., ADJ\(\rightarrow\)NOUN) with emissions.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\linewidth]{ngram_vs_hmm.png}
  \caption{Token accuracy: default/unigram/bigram/trigram vs. HMMs (Basque split).}
  \label{fig:ngram}
\end{figure}

\subsection{Per-tag analysis}
Figures~\ref{fig:per-tag-eu} and \ref{fig:per-tag-ca} detail per-tag accuracies for our HMM. Although the bars vary by tag, the quantitative pattern is consistent: closed classes (DET, ADP, AUX, PUNCT) cluster at the top, while open classes (NOUN, VERB, PROPN, ADV) drop. In Basque the drop is sharper due to larger vocabulary and sparser emissions; in Catalan the spread is milder.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{pos_accuracy_eu.png}
  \caption{Per-tag accuracy of our HMM on the Basque test set.}
  \label{fig:per-tag-eu}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.95\linewidth]{pos_accuracy_ca.png}
  \caption{Per-tag accuracy of our HMM on the Catalan test set.}
  \label{fig:per-tag-ca}
\end{figure}

\subsection{Per-tag bars and comparisons}
Grouped bar charts from the classification reports summarize precision/recall/F1 by tag. Figures~\ref{fig:per-tag-bars-nltk-eu}–\ref{fig:per-tag-bars-nltk-ca} (NLTK) and \ref{fig:per-tag-bars-ours-eu}–\ref{fig:per-tag-bars-ours-ca} (ours) show how scores spread across tags; Figures~\ref{fig:per-tag-compare-eu}–\ref{fig:per-tag-compare-ca} compare F1 directly between models. Our HMM lifts most tags over NLTK in both languages, matching the macro-F1 gains reported numerically. Full confusion matrices are provided in Appendix~\ref{app:confusion}.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{per_tag_nltk_eu.png}
  \caption{Per-tag precision/recall/F1 (NLTK HMM, Basque). Higher variance across tags.}
  \label{fig:per-tag-bars-nltk-eu}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{per_tag_nltk_ca.png}
  \caption{Per-tag precision/recall/F1 (NLTK HMM, Catalan).}
  \label{fig:per-tag-bars-nltk-ca}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{per_tag_our_eu.png}
  \caption{Per-tag precision/recall/F1 (Our HMM, Basque). Closed classes dominate; open classes drop.}
  \label{fig:per-tag-bars-ours-eu}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{per_tag_our_ca.png}
  \caption{Per-tag precision/recall/F1 (Our HMM, Catalan). More uniform than Basque.}
  \label{fig:per-tag-bars-ours-ca}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{compare_reports_eu.png}
  \caption{Per-tag F1 comparison: Our HMM vs NLTK (Basque). Gains concentrated in frequent tags.}
  \label{fig:per-tag-compare-eu}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\linewidth]{compare_reports_ca.png}
  \caption{Per-tag F1 comparison: Our HMM vs NLTK (Catalan). Broad improvements with small gaps.}
  \label{fig:per-tag-compare-ca}
\end{figure}

\subsection{Qualitative checks}
The joint probability for a Basque test sentence was \(4.29\times 10^{-13}\), consistent with multiplying probabilities over long sequences. Random samples from the HMM show plausible tag alternations (ADJ\(\rightarrow\)NOUN\(\rightarrow\)VERB), reinforcing the generative interpretation.

\subsection{Basque test metrics}
Running \texttt{main.py} on the Basque test set yields 0.85595 accuracy. Table~\ref{tab:eu-metrics} summarizes precision/recall/F1 by tag; macro-F1 is 0.772 (macro recall 0.745) and weighted-F1 0.854, reflecting drops in rare classes (INTJ, SCONJ, X) and stability in frequent/closed ones (PUNCT, PART, CCONJ).

\begin{table}[H]
  \centering
  \caption{Basque test set: precision/recall/F1 by tag.}
  \label{tab:eu-metrics}
  \small
  \begin{tabular}{lccc}
    \toprule
    Tag & Precision & Recall & F1 \\
    \midrule
    ADJ   & 0.914 & 0.626 & 0.743 \\
    ADP   & 0.879 & 0.911 & 0.895 \\
    ADV   & 0.939 & 0.818 & 0.874 \\
    AUX   & 0.773 & 0.900 & 0.832 \\
    CCONJ & 0.956 & 0.986 & 0.971 \\
    DET   & 0.959 & 0.917 & 0.938 \\
    INTJ  & 0.500 & 0.125 & 0.200 \\
    NOUN  & 0.798 & 0.853 & 0.824 \\
    NUM   & 0.996 & 0.781 & 0.876 \\
    PART  & 0.990 & 0.997 & 0.993 \\
    PRON  & 1.000 & 0.827 & 0.906 \\
    PROPN & 0.831 & 0.662 & 0.737 \\
    PUNCT & 0.936 & 1.000 & 0.967 \\
    SCONJ & 0.000 & 0.000 & 0.000 \\
    SYM   & 1.000 & 1.000 & 1.000 \\
    VERB  & 0.812 & 0.823 & 0.817 \\
    X     & 0.769 & 0.435 & 0.556 \\
    \midrule
    \textbf{Macro-F1}     & \multicolumn{3}{c}{0.772 (macro recall 0.745)} \\
    \textbf{Weighted-F1}  & \multicolumn{3}{c}{0.854} \\
    \textbf{Accuracy}     & \multicolumn{3}{c}{0.85595} \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Catalan test metrics}
Running \texttt{main.py} on the Catalan test set yields 0.94479 accuracy. Table~\ref{tab:ca-metrics} summarizes precision/recall/F1 by tag; macro-F1 is 0.836 (macro recall 0.827) and weighted-F1 0.944, showing higher overall performance and fewer drops on rare classes compared to Basque.

\begin{table}[H]
  \centering
  \caption{Catalan test set: precision/recall/F1 by tag.}
  \label{tab:ca-metrics}
  \small
  \begin{tabular}{lccc}
    \toprule
    Tag & Precision & Recall & F1 \\
    \midrule
    ADJ   & 0.906 & 0.870 & 0.888 \\
    ADP   & 0.980 & 0.993 & 0.986 \\
    ADV   & 0.927 & 0.949 & 0.938 \\
    AUX   & 0.949 & 0.985 & 0.967 \\
    CCONJ & 0.988 & 0.992 & 0.990 \\
    DET   & 0.951 & 0.990 & 0.970 \\
    INTJ  & 0.000 & 0.000 & 0.000 \\
    NOUN  & 0.928 & 0.953 & 0.940 \\
    NUM   & 0.922 & 0.824 & 0.870 \\
    PART  & 1.000 & 0.190 & 0.320 \\
    PRON  & 0.907 & 0.851 & 0.878 \\
    PROPN & 0.924 & 0.832 & 0.876 \\
    PUNCT & 0.984 & 0.992 & 0.988 \\
    SCONJ & 0.776 & 0.916 & 0.840 \\
    SYM   & 0.990 & 0.981 & 0.986 \\
    VERB  & 0.954 & 0.919 & 0.936 \\
    \midrule
    \textbf{Macro-F1}     & \multicolumn{3}{c}{0.836 (macro recall 0.827)} \\
    \textbf{Weighted-F1}  & \multicolumn{3}{c}{0.944} \\
    \textbf{Accuracy}     & \multicolumn{3}{c}{0.94479} \\
    \bottomrule
  \end{tabular}
\end{table}
